#!/usr/bin/env python3
"""
–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞ —Ä–æ–±–æ—Ç–∞: –†–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è ML-–º–æ–¥–µ–ª—ñ –¥–ª—è Speech Commands (PyTorch + Flask)

–¶–µ–π —Å–∫—Ä–∏–ø—Ç –≥–æ—Ç—É—î –ø—Ä–æ—Å—Ç—É CNN-–º–æ–¥–µ–ª—å –¥–ª—è –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∏ –∫–æ–º–∞–Ω–¥ (yes, no, up, down),
–Ω–∞–≤—á–∞—î —ó—ó, –æ—Ü—ñ–Ω—é—î Accuracy, –≤–∏–º—ñ—Ä—é—î Latency, —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞—î –≤–∞–≥–∏ –¥–ª—è –ø–æ–¥–∞–ª—å—à–æ–≥–æ
—ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É —É Flask API.

–ë–∞–∑—É—î—Ç—å—Å—è –Ω–∞ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–º—É Jupyter –Ω–æ—É—Ç–±—É—Ü—ñ –∑ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–∏–º–∏ –ø–æ–º–∏–ª–∫–∞–º–∏.
"""

import os
import time
import math
import random
import warnings
from pathlib import Path
from typing import List, Tuple
from datetime import datetime
from collections import Counter
import argparse
import json
import csv

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset
import torchaudio
from torchaudio.datasets import SPEECHCOMMANDS

from model_utils import SmallCNN, wav_to_melspec, SAMPLE_RATE, measure_latency, save_model, export_torchscript

# –í—ñ–¥–∫–ª—é—á–∞—î–º–æ –ø–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è –¥–ª—è —á–∏—Å—Ç–æ–≥–æ –≤–∏–≤–æ–¥—É
warnings.filterwarnings("ignore", category=UserWarning)


def print_header(title: str):
    """–ö—Ä–∞—Å–∏–≤–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫"""
    print("\n" + "=" * 60)
    print(f"üéØ {title}")
    print("=" * 60)


def print_progress(current: int, total: int, prefix: str = "Progress"):
    """–ü—Ä–æ—Å—Ç–∏–π —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–≥—Ä–µ—Å—É"""
    percent = 100 * current / total
    bar_length = 30
    filled_length = int(bar_length * current // total)
    bar = "‚ñà" * filled_length + "-" * (bar_length - filled_length)
    print(f"\r{prefix}: |{bar}| {percent:.1f}% ({current}/{total})", end="", flush=True)


# –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç—ñ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–≥–æ —Ä—è–¥–∫–∞
parser = argparse.ArgumentParser(description="Train Speech Commands model")
parser.add_argument("--output-dir", type=str, default=".",
                    help="Directory to save model and metrics")
parser.add_argument("--epochs", type=int, default=3,
                    help="Number of training epochs")
parser.add_argument("--batch-size", type=int, default=32,
                    help="Batch size for training")
args = parser.parse_args()

# –°—Ç–≤–æ—Ä—é—î–º–æ –≤–∏—Ö—ñ–¥–Ω—É –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é
OUTPUT_DIR = args.output_dir
os.makedirs(OUTPUT_DIR, exist_ok=True)

print_header("SPEECH COMMANDS CLASSIFICATION")
print("üéµ –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞ —Ä–æ–±–æ—Ç–∞: –†–æ–∑–≥–æ—Ä—Ç–∞–Ω–Ω—è ML-–º–æ–¥–µ–ª—ñ")
print(f"üìÖ –ó–∞–ø—É—Å–∫: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"üìÅ –í–∏—Ö—ñ–¥–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è: {OUTPUT_DIR}")

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—ñ CUDA
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üì± –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø—Ä–∏—Å—Ç—Ä—ñ–π: {device}")
if torch.cuda.is_available():
    print(f"üî• GPU: {torch.cuda.get_device_name(0)}")
    print(f"üíæ GPU –ø–∞–º'—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory // 1024 ** 3} GB")
else:
    print("üíª –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è CPU")

# ============================================================================
# –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø
# ============================================================================
print_header("–ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø")

# –í–ê–ñ–õ–ò–í–û: –î–ª—è Docker runtime –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ 2 –∫–ª–∞—Å–∏ –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
# –î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–∂–µ—Ç–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –±—ñ–ª—å—à–µ –∫–ª–∞—Å—ñ–≤
CLASSES = [
    "yes", "no", "up", "down"
    """
     , "zero", "wow", "visual", "two", "tree", "three",
     "stop", "six", "sheila", "seven", "right", "one", "off", "nine", "marvin",
     "left", "learn", "house", "happy", "go", "four", "forward", "follow", "five",
     "eight", "dog", "cat", "bird", "bed", "backward"
    """
]
N_CLASSES = len(CLASSES)
BATCH_SIZE = args.batch_size
EPOCHS = args.epochs
LR = 1e-3
DATA_ROOT = os.path.join(os.getcwd(), "data_speech")
MODEL_PATH = os.path.join(OUTPUT_DIR, "model_state_dict.pt")
TS_PATH = os.path.join(OUTPUT_DIR, "model_scripted.pt")
METRICS_JSON = os.path.join(OUTPUT_DIR, "metrics.json")
METRICS_CSV = os.path.join(OUTPUT_DIR, "metrics.csv")
TRAINING_LOG = os.path.join(OUTPUT_DIR, "training.log")

os.makedirs(DATA_ROOT, exist_ok=True)

print(f"üéØ –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è:")
print(f"   - –ö–ª–∞—Å–∏: {CLASSES}")
print(f"   - –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤: {N_CLASSES}")
print(f"   - Batch size: {BATCH_SIZE}")
print(f"   - Epochs: {EPOCHS}")
print(f"   - Learning rate: {LR}")
print(f"   - –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–∞–Ω–∏—Ö: {DATA_ROOT}")
print(f"   - –ú–æ–¥–µ–ª—å –±—É–¥–µ –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {MODEL_PATH}")


# ============================================================================
# DATASET
# ============================================================================

class SubsetSpeechCommands(SPEECHCOMMANDS):
    """–û–±–≥–æ—Ä—Ç–∫–∞ –Ω–∞–¥ –¥–∞—Ç–∞—Å–µ—Ç–æ–º –¥–ª—è –≤–∏–±—ñ—Ä–∫–∏ —Ç—ñ–ª—å–∫–∏ –ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö –∫–ª–∞—Å—ñ–≤"""

    def __init__(self, root: str, subset: str = None, target_classes: List[str] = None):
        print(f"üìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ {subset} –ø—ñ–¥–º–Ω–æ–∂–∏–Ω—É Speech Commands...")
        super().__init__(root, download=True, subset=subset)
        self.target_classes = set(target_classes) if target_classes else None
        self.label2idx = {c: i for i, c in enumerate(target_classes)} if target_classes else None

        # –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä—É—î–º–æ —ñ–Ω–¥–µ–∫—Å–∏ –∑ –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏–º–∏ –∫–ª–∞—Å–∞–º–∏
        self._filtered = []
        print(f"üîç –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∫–ª–∞—Å—ñ–≤ {target_classes}...")

        for i in range(len(self._walker)):
            path = self._walker[i]
            label = Path(path).parent.name
            if self.target_classes is None or label in self.target_classes:
                self._filtered.append(i)

        print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(self._filtered)} —Ñ–∞–π–ª—ñ–≤ –¥–ª—è –∫–ª–∞—Å—ñ–≤ {target_classes}")

    def __len__(self):
        return len(self._filtered)

    def __getitem__(self, idx: int):
        actual_idx = self._filtered[idx]
        waveform, sample_rate, label, *_ = super().__getitem__(actual_idx)
        y = self.label2idx[label] if self.label2idx else label
        # –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è —É Mel-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º—É (–≤ dB) + –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è ~[0..1]
        spec = wav_to_melspec(waveform, sample_rate)  # [1, n_mels, T]
        spec = (spec + 80.0) / 80.0
        return spec, y


def collate_fn(batch):
    """–§—É–Ω–∫—Ü—ñ—è –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –±–∞—Ç—á—ñ–≤ –∑ padding"""
    specs, labels = zip(*batch)  # —Å–ø–∏—Å–æ–∫ —Ç–µ–Ω–∑–æ—Ä—ñ–≤ [1, n_mels, T_i]
    max_T = max(s.size(-1) for s in specs)
    padded = []
    for s in specs:
        if s.size(-1) < max_T:
            pad = torch.nn.functional.pad(s, (0, max_T - s.size(-1)))
            padded.append(pad)
        else:
            padded.append(s)
    x = torch.stack(padded, dim=0)  # [B, 1, n_mels, T]
    y = torch.tensor(labels, dtype=torch.long)
    return x, y


# ============================================================================
# –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –î–ê–ù–ò–•
# ============================================================================
print_header("–ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –î–ê–ù–ò–•")

try:
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è train/test (—É SPEECHCOMMANDS —î —Ñ—ñ–∫—Å–æ–≤–∞–Ω—ñ –ø—ñ–¥–º–Ω–æ–∂–∏–Ω–∏)
    train_ds = SubsetSpeechCommands(DATA_ROOT, subset="training", target_classes=CLASSES)
    test_ds = SubsetSpeechCommands(DATA_ROOT, subset="testing", target_classes=CLASSES)

    print(f"üìä –†–æ–∑–º—ñ—Ä –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤:")
    print(f"   - Train: {len(train_ds):,} –∑—Ä–∞–∑–∫—ñ–≤")
    print(f"   - Test: {len(test_ds):,} –∑—Ä–∞–∑–∫—ñ–≤")

    # –°—Ç–≤–æ—Ä—é—î–º–æ data loaders –∑ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–º–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è–º–∏
    print(f"‚öôÔ∏è –°—Ç–≤–æ—Ä—é—î–º–æ data loaders...")
    print(f"   - Batch size: {BATCH_SIZE}")
    print(f"   - –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –¥–ª—è {'GPU' if torch.cuda.is_available() else 'CPU'}")

    train_loader = DataLoader(
        train_ds,
        batch_size=BATCH_SIZE,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0,  # 0 –¥–ª—è Windows, —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –ø—Ä–æ–±–ª–µ–º –∑ multiprocessing
        pin_memory=torch.cuda.is_available(),  # True –¥–ª—è GPU, False –¥–ª—è CPU
        persistent_workers=False
    )

    test_loader = DataLoader(
        test_ds,
        batch_size=BATCH_SIZE,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0,
        pin_memory=torch.cuda.is_available(),
        persistent_workers=False
    )

    print(f"‚úÖ Data loaders —Å—Ç–≤–æ—Ä–µ–Ω–æ:")
    print(f"   - Train batches: {len(train_loader)}")
    print(f"   - Test batches: {len(test_loader)}")

except Exception as e:
    print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö: {e}")
    exit(1)

# ============================================================================
# –°–¢–í–û–†–ï–ù–ù–Ø –ú–û–î–ï–õ–Ü
# ============================================================================
print_header("–°–¢–í–û–†–ï–ù–ù–Ø –ú–û–î–ï–õ–Ü")

try:
    # –ú–æ–¥–µ–ª—å, –ª–æ—Å—Å, –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä
    model = SmallCNN(n_classes=N_CLASSES).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LR)

    # –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –º–æ–¥–µ–ª—å
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"üß† –ú–æ–¥–µ–ª—å: {model.__class__.__name__}")
    print(f"üî¢ –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤: {total_params:,}")
    print(f"üéØ –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤: {trainable_params:,}")
    print(f"üìê –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞:")
    print(f"   - –í—Ö—ñ–¥: [batch, 1, n_mels, time]")
    print(f"   - –í–∏—Ö—ñ–¥: [batch, {N_CLASSES}] (–ª–æ–≥—ñ—Ç–∏ –¥–ª—è {N_CLASSES} –∫–ª–∞—Å—ñ–≤)")
    print(f"   - –§—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç: CrossEntropyLoss")
    print(f"   - –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä: Adam (lr={LR})")

    print(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø—ñ—à–Ω–æ —Å—Ç–≤–æ—Ä–µ–Ω–∞ —Ç–∞ –ø–µ—Ä–µ–º—ñ—â–µ–Ω–∞ –Ω–∞ {device}")

except Exception as e:
    print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
    exit(1)

# ============================================================================
# –¢–†–ï–ù–£–í–ê–ù–ù–Ø
# ============================================================================
print_header("–¢–†–ï–ù–£–í–ê–ù–ù–Ø –ú–û–î–ï–õ–Ü")

print(f"üöÄ –ü–æ—á–∏–Ω–∞—î–º–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è...")
print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
print(f"   - –ï–ø–æ—Ö–∏: {EPOCHS}")
print(f"   - –ë–∞—Ç—á—ñ–≤ –Ω–∞ –µ–ø–æ—Ö—É: {len(train_loader)}")
print(f"   - –ó—Ä–∞–∑–∫—ñ–≤ –Ω–∞ –µ–ø–æ—Ö—É: {len(train_ds):,}")
print(f"   - –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –±–∞—Ç—á—ñ–≤: {EPOCHS * len(train_loader)}")

start_time = time.time()

try:
    for epoch in range(1, EPOCHS + 1):
        epoch_start = time.time()
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        print(f"\nüìà Epoch {epoch}/{EPOCHS} - {datetime.now().strftime('%H:%M:%S')}")

        for batch_idx, (x, y) in enumerate(train_loader):
            # –ü–µ—Ä–µ–º—ñ—â—É—î–º–æ –¥–∞–Ω—ñ –Ω–∞ –ø—Ä–∏—Å—Ç—Ä—ñ–π
            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)

            # Forward pass
            optimizer.zero_grad()
            logits = model(x)
            loss = criterion(logits, y)

            # Backward pass
            loss.backward()
            optimizer.step()

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            running_loss += loss.item() * x.size(0)
            pred = logits.argmax(dim=1)
            correct += (pred == y).sum().item()
            total += y.size(0)

            # –ü–æ–∫–∞–∑—É—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å –∫–æ–∂–Ω—ñ 50 –±–∞—Ç—á—ñ–≤
            if batch_idx % 50 == 0:
                current_acc = correct / total if total > 0 else 0
                print(f"   Batch {batch_idx + 1:3d}/{len(train_loader)} "
                      f"({100 * batch_idx / len(train_loader):5.1f}%) - "
                      f"Loss: {loss.item():.4f}, Acc: {current_acc:.4f}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –µ–ø–æ—Ö–∏
        epoch_time = time.time() - epoch_start
        train_loss = running_loss / total
        train_acc = correct / total

        print(f"‚úÖ Epoch {epoch} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {epoch_time:.1f}—Å")
        print(f"   üìâ –°–µ—Ä–µ–¥–Ω—è –≤—Ç—Ä–∞—Ç–∞: {train_loss:.4f}")
        print(f"   üéØ –¢–æ—á–Ω—ñ—Å—Ç—å: {train_acc * 100:.2f}%")
        print(f"   ‚ö° –®–≤–∏–¥–∫—ñ—Å—Ç—å: {total / epoch_time:.1f} –∑—Ä–∞–∑–∫—ñ–≤/—Å–µ–∫")

        # –õ–æ–≥—É–≤–∞–Ω–Ω—è –º–µ—Ç—Ä–∏–∫ —É JSON —Ç–∞ CSV
        try:
            metrics = {
                "epoch": epoch,
                "train_loss": train_loss,
                "train_accuracy": train_acc,
                "learning_rate": LR
            }

            # JSON –ª–æ–≥—É–≤–∞–Ω–Ω—è
            with open(METRICS_JSON, "a") as json_file:
                json.dump(metrics, json_file)
                json_file.write("\n")

            # CSV –ª–æ–≥—É–≤–∞–Ω–Ω—è
            with open(METRICS_CSV, "a", newline="") as csv_file:
                fieldnames = metrics.keys()
                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)

                # –ó–∞–ø–∏—Å—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫, —è–∫—â–æ —Ñ–∞–π–ª –Ω–æ–≤–∏–π
                if csv_file.tell() == 0:
                    writer.writeheader()

                writer.writerow(metrics)

        except Exception as e:
            print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ª–æ–≥—É–≤–∞–Ω–Ω—è –º–µ—Ç—Ä–∏–∫: {e}")

    total_time = time.time() - start_time
    print(f"\nüéâ –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"‚è±Ô∏è –ó–∞–≥–∞–ª—å–Ω–∏–π —á–∞—Å: {total_time:.1f} —Å–µ–∫—É–Ω–¥ ({total_time / 60:.1f} —Ö–≤–∏–ª–∏–Ω)")
    print(f"üìä –§—ñ–Ω–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ train: {train_acc * 100:.2f}%")

except KeyboardInterrupt:
    print(f"\n‚èπÔ∏è –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –ø–µ—Ä–µ—Ä–≤–∞–Ω–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º")
except Exception as e:
    print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: {e}")
    import traceback

    traceback.print_exc()
    exit(1)

# ============================================================================
# –û–¶–Ü–ù–ö–ê –ù–ê TEST SET
# ============================================================================
print_header("–û–¶–Ü–ù–ö–ê –ú–û–î–ï–õ–Ü –ù–ê –¢–ï–°–¢–û–í–û–ú–£ –ù–ê–ë–û–†–Ü")

print(f"üîç –û—Ü—ñ–Ω—é—î–º–æ –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ...")
print(f"üìä –¢–µ—Å—Ç–æ–≤–∏—Ö –∑—Ä–∞–∑–∫—ñ–≤: {len(test_ds):,}")
print(f"üì¶ –¢–µ—Å—Ç–æ–≤–∏—Ö –±–∞—Ç—á—ñ–≤: {len(test_loader)}")

eval_start = time.time()

try:
    model.eval()
    correct = 0
    total = 0
    all_preds = []
    all_labels = []
    test_loss = 0.0

    with torch.no_grad():
        for batch_idx, (x, y) in enumerate(test_loader):
            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)

            # Forward pass
            logits = model(x)
            loss = criterion(logits, y)

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            test_loss += loss.item() * x.size(0)
            pred = logits.argmax(dim=1)
            correct += (pred == y).sum().item()
            total += y.size(0)

            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É
            all_preds.extend(pred.cpu().numpy())
            all_labels.extend(y.cpu().numpy())

            # –ü—Ä–æ–≥—Ä–µ—Å –∫–æ–∂–Ω—ñ 10 –±–∞—Ç—á—ñ–≤
            if batch_idx % 10 == 0:
                print_progress(batch_idx + 1, len(test_loader), "   –û—Ü—ñ–Ω–∫–∞")

    print()  # –ù–æ–≤–∏–π —Ä—è–¥–æ–∫ –ø—ñ—Å–ª—è –ø—Ä–æ–≥—Ä–µ—Å—É

    eval_time = time.time() - eval_start
    test_loss = test_loss / total
    test_acc = correct / total

    print(f"‚úÖ –û—Ü—ñ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {eval_time:.1f}—Å")
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç–∏:")
    print(f"   üéØ Test Accuracy: {test_acc * 100:.2f}%")
    print(f"   üìâ Test Loss: {test_loss:.4f}")
    print(f"   ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∏—Ö –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å: {correct:,}/{total:,}")
    print(f"   ‚ö° –®–≤–∏–¥–∫—ñ—Å—Ç—å –æ—Ü—ñ–Ω–∫–∏: {total / eval_time:.1f} –∑—Ä–∞–∑–∫—ñ–≤/—Å–µ–∫")

    # –î–µ—Ç–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å–∞—Ö
    print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å–∞—Ö:")
    pred_counter = Counter(all_preds)
    label_counter = Counter(all_labels)

    for i, class_name in enumerate(CLASSES):
        true_count = label_counter.get(i, 0)
        pred_count = pred_counter.get(i, 0)

        # –¢–æ—á–Ω—ñ—Å—Ç—å –¥–ª—è —Ü—å–æ–≥–æ –∫–ª–∞—Å—É
        class_correct = sum(1 for p, l in zip(all_preds, all_labels) if p == i and l == i)
        class_acc = class_correct / true_count if true_count > 0 else 0

        print(f"   {class_name:>6}: {true_count:4d} —Å–ø—Ä–∞–≤–∂–Ω—ñ—Ö, "
              f"{pred_count:4d} –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∏—Ö, —Ç–æ—á–Ω—ñ—Å—Ç—å: {class_acc * 100:5.1f}%")

    # –õ–æ–≥—É–≤–∞–Ω–Ω—è –º–µ—Ç—Ä–∏–∫ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    try:
        metrics = {
            "epoch": EPOCHS,  # –û—Å—Ç–∞–Ω–Ω—è –µ–ø–æ—Ö–∞
            "test_loss": test_loss,
            "test_accuracy": test_acc
        }

        # JSON –ª–æ–≥—É–≤–∞–Ω–Ω—è
        with open(METRICS_JSON, "a") as json_file:
            json.dump(metrics, json_file)
            json_file.write("\n")

        # CSV –ª–æ–≥—É–≤–∞–Ω–Ω—è
        with open(METRICS_CSV, "a", newline="") as csv_file:
            fieldnames = metrics.keys()
            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)

            # –ó–∞–ø–∏—Å—É—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫, —è–∫—â–æ —Ñ–∞–π–ª –Ω–æ–≤–∏–π
            if csv_file.tell() == 0:
                writer.writeheader()

            writer.writerow(metrics)

    except Exception as e:
        print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ª–æ–≥—É–≤–∞–Ω–Ω—è –º–µ—Ç—Ä–∏–∫ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è: {e}")

except Exception as e:
    print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –æ—Ü—ñ–Ω–∫–∏: {e}")
    import traceback

    traceback.print_exc()

# ============================================================================
# –í–ò–ú–Ü–†–Æ–í–ê–ù–ù–Ø –õ–ê–¢–ï–ù–¢–ù–û–°–¢–Ü
# ============================================================================
print_header("–í–ò–ú–Ü–†–Æ–í–ê–ù–ù–Ø –ü–†–û–î–£–ö–¢–ò–í–ù–û–°–¢–Ü")

print(f"‚è±Ô∏è –í–∏–º—ñ—Ä—é—î–º–æ –ª–∞—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É...")

try:
    # –ü—ñ–¥–≥–æ—Ç—É—î–º–æ –ø—Ä–∏–∫–ª–∞–¥ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    example_spec, _ = test_ds[0]

    # –ü–∞–¥–∏–º–æ –¥–æ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—ó –¥–æ–≤–∂–∏–Ω–∏ (–±–µ—Ä–µ–º–æ –º–∞–∫—Å–∏–º—É–º –∑ –∫—ñ–ª—å–∫–æ—Ö –∑—Ä–∞–∑–∫—ñ–≤)
    test_samples = [test_ds[i] for i in range(min(8, len(test_ds)))]
    ex_T = max(s.size(-1) for s, _ in test_samples) if test_samples else example_spec.size(-1)

    if example_spec.size(-1) < ex_T:
        example_spec = torch.nn.functional.pad(example_spec, (0, ex_T - example_spec.size(-1)))

    example = example_spec.unsqueeze(0).to(device)  # [1,1,n_mels,T]

    print(f"üìè –†–æ–∑–º—ñ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –≤—Ö–æ–¥—É: {list(example.shape)}")
    print(f"üî¨ –í–∏–º—ñ—Ä—é—î–º–æ –ª–∞—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å –Ω–∞ 30 –ø—Ä–æ–≥–æ–Ω–∞—Ö...")

    lat_ms = measure_latency(model, example, runs=30)

    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ:")
    print(f"   ‚ö° –°–µ—Ä–µ–¥–Ω—è –ª–∞—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å: {lat_ms:.2f} –º—Å")
    print(f"   üöÄ –ü—Ä–æ–ø—É—Å–∫–Ω–∞ –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å: {1000 / lat_ms:.1f} –∑–∞–ø–∏—Ç—ñ–≤/—Å–µ–∫")
    print(f"   üìä –ß–∞—Å –Ω–∞ –æ–¥–∏–Ω –∑—Ä–∞–∑–æ–∫: {lat_ms:.2f} –º—Å")

    if lat_ms < 100:
        print(f"   üü¢ –í—ñ–¥–º—ñ–Ω–Ω–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ —á–∞—Å—É!")
    elif lat_ms < 500:
        print(f"   üü° –•–æ—Ä–æ—à–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å –¥–ª—è –±—ñ–ª—å—à–æ—Å—Ç—ñ –∑–∞—Å—Ç–æ—Å—É–Ω–∫—ñ–≤")
    else:
        print(f"   üî¥ –ú–æ–∂–ª–∏–≤–æ –∑–Ω–∞–¥–æ–±–∏—Ç—å—Å—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ —á–∞—Å—É")

except Exception as e:
    print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ: {e}")
    # –°—Ç–≤–æ—Ä—é—î–º–æ dummy –ø—Ä–∏–∫–ª–∞–¥
    example = torch.randn(1, 1, 64, 32).to(device)
    lat_ms = 0.0

# ============================================================================
# –ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø –ú–û–î–ï–õ–Ü
# ============================================================================
print_header("–ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø –ú–û–î–ï–õ–ï–ô")

print(f"üíæ –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω—É –º–æ–¥–µ–ª—å...")

try:
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è state_dict
    size_sd = save_model(model, MODEL_PATH)
    print(f"‚úÖ State dict –∑–±–µ—Ä–µ–∂–µ–Ω–æ:")
    print(f"   üìÅ –§–∞–π–ª: {MODEL_PATH}")
    print(f"   üìè –†–æ–∑–º—ñ—Ä: {size_sd / 1024:.1f} KB")

    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è TorchScript
    try:
        size_ts = export_torchscript(model, example, TS_PATH)
        print(f"‚úÖ TorchScript –∑–±–µ—Ä–µ–∂–µ–Ω–æ:")
        print(f"   üìÅ –§–∞–π–ª: {TS_PATH}")
        print(f"   üìè –†–æ–∑–º—ñ—Ä: {size_ts / 1024:.1f} KB")
        print(f"   üìä –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç —Å—Ç–∏—Å–Ω–µ–Ω–Ω—è: {size_ts / size_sd:.2f}x")
    except Exception as e:
        print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è TorchScript: {e}")
        print(f"   State dict –≤—Å–µ –æ–¥–Ω–æ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —ñ –º–æ–∂–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏—Å—å")

    print(f"üí° –§–∞–π–ª–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ –ø–æ—Ç–æ—á–Ω—ñ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó:")
    print(f"   {os.path.abspath(MODEL_PATH)}")
    if os.path.exists(TS_PATH):
        print(f"   {os.path.abspath(TS_PATH)}")

except Exception as e:
    print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è: {e}")

# ============================================================================
# –ü–Ü–î–°–£–ú–û–ö
# ============================================================================
print_header("–ü–Ü–î–°–£–ú–û–ö –†–û–ë–û–¢–ò")

print(f"üéâ –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞ —Ä–æ–±–æ—Ç–∞ —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
print(f"üìä –§—ñ–Ω–∞–ª—å–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏:")
if 'test_acc' in locals():
    print(f"   üéØ –¢–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç—ñ: {test_acc * 100:.2f}%")
if 'lat_ms' in locals():
    print(f"   ‚ö° –õ–∞—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å: {lat_ms:.2f} –º—Å")
print(f"   üíæ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {MODEL_PATH}")
print(f"   üéµ –ö–ª–∞—Å–∏: {CLASSES}")
print(f"   üì± –ü—Ä–∏—Å—Ç—Ä—ñ–π: {device}")

print(f"\nüìù –Ø–∫ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –∑ Flask API:")
print(f"   1. –ü–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—è, —â–æ —Ñ–∞–π–ª–∏ model_utils.py —Ç–∞ {MODEL_PATH} –≤ –ø–æ—Ç–æ—á–Ω—ñ–π –ø–∞–ø—Ü—ñ")
print(f"   2. –ó–∞–ø—É—Å—Ç—ñ—Ç—å API: python app.py")
print(f"   3. –¢–µ—Å—Ç—É–π—Ç–µ:")
print(f"      curl -X POST -F \"file=@sample.wav\" http://127.0.0.1:8000/predict")

print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:")
if 'test_acc' in locals():
    if test_acc > 0.85:
        print(f"   üü¢ –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—É—î —Ö–æ—Ä–æ—à—É —Ç–æ—á–Ω—ñ—Å—Ç—å!")
    elif test_acc > 0.70:
        print(f"   üü° –ú–æ–¥–µ–ª—å –ø—Ä–∞—Ü—é—î –∑–∞–¥–æ–≤—ñ–ª—å–Ω–æ. –ú–æ–∂–Ω–∞ –ø–æ–∫—Ä–∞—â–∏—Ç–∏ –∑–±—ñ–ª—å—à–∏–≤—à–∏ EPOCHS")
    else:
        print(f"   üî¥ –ú–æ–¥–µ–ª—å –ø–æ—Ç—Ä–µ–±—É—î –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è. –°–ø—Ä–æ–±—É–π—Ç–µ –±—ñ–ª—å—à–µ –µ–ø–æ—Ö –∞–±–æ —ñ–Ω—à—ñ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏")

if 'lat_ms' in locals() and lat_ms > 0:
    if lat_ms < 50:
        print(f"   üü¢ –í—ñ–¥–º—ñ–Ω–Ω–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å –¥–ª—è real-time –∑–∞—Å—Ç–æ—Å—É–Ω–∫—ñ–≤!")
    elif lat_ms < 200:
        print(f"   üü° –•–æ—Ä–æ—à–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å –¥–ª—è –±—ñ–ª—å—à–æ—Å—Ç—ñ –∑–∞—Å—Ç–æ—Å—É–Ω–∫—ñ–≤")

print(f"\n‚úÖ –í—Å—ñ –µ—Ç–∞–ø–∏ –≤–∏–∫–æ–Ω–∞–Ω–æ:")
print(f"   üìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö ‚úÖ")
print(f"   üß† –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ ‚úÖ")
print(f"   üèãÔ∏è –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è ‚úÖ")
print(f"   üéØ –û—Ü—ñ–Ω–∫–∞ ‚úÖ")
print(f"   ‚ö° –í–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ ‚úÖ")
print(f"   üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è ‚úÖ")

print(f"\nüéµ Speech Commands Classification –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
print(f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("=" * 60)