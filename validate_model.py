#!/usr/bin/env python3
"""
–í–∞–ª—ñ–¥–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ –Ω–∞ —Å—Ç–∞–±—ñ–ª—å–Ω–æ–º—É —Ç–µ—Å—Ç–æ–≤–æ–º—É –∑—Ä–∞–∑–∫—É
"""

import argparse
import json
import os
import warnings

import torch
import numpy as np

from model_utils import SmallCNN, wav_to_melspec, load_model, get_model_metadata

warnings.filterwarnings("ignore", category=UserWarning)


def validate_model_consistency(model_path: str):
    """
    –ü–µ—Ä–µ–≤—ñ—Ä—è—î –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ –Ω–∞ —Å—Ç–∞–±—ñ–ª—å–Ω–æ–º—É –∑—Ä–∞–∑–∫—É
    """
    print("=" * 60)
    print("üîç –í–ê–õ–Ü–î–ê–¶–Ü–Ø –ú–û–î–ï–õ–Ü")
    print("=" * 60)

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞: {model_path}")

    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–Ω–∞–π–¥–µ–Ω–∞: {model_path}")
    print(f"üìè –†–æ–∑–º—ñ—Ä –º–æ–¥–µ–ª—ñ: {os.path.getsize(model_path) / 1024 / 1024:.2f} MB")

    # –ß–∏—Ç–∞—î–º–æ –º–µ—Ç–∞–¥–∞–Ω—ñ –º–æ–¥–µ–ª—ñ
    print(f"üìñ –ß–∏—Ç–∞–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö –º–æ–¥–µ–ª—ñ...")
    metadata = get_model_metadata(model_path)
    n_classes = metadata['n_classes']
    class_names = metadata['class_names']

    if n_classes is None:
        raise ValueError("–ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤ –∑ –º–æ–¥–µ–ª—ñ. –ú–æ–¥–µ–ª—å –º–æ–∂–µ –±—É—Ç–∏ –ø–æ—à–∫–æ–¥–∂–µ–Ω–∞.")

    print(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω—ñ –∑—á–∏—Ç–∞–Ω–æ:")
    print(f"   - –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—ñ–≤: {n_classes}")
    print(f"   - –ù–∞–∑–≤–∏ –∫–ª–∞—Å—ñ–≤: {'–¢–∞–∫' if class_names else '–ù—ñ'}")

    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –Ω–∞–∑–≤–∏ –∫–ª–∞—Å—ñ–≤ –∞–±–æ —Å—Ç–≤–æ—Ä—é—î–º–æ generic
    if class_names:
        CLASSES = class_names
    else:
        CLASSES = [f"class_{i}" for i in range(n_classes)]
        print(f"‚ö†Ô∏è  –ù–∞–∑–≤–∏ –∫–ª–∞—Å—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω—ñ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ generic: {CLASSES}")

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ (n_classes –±—É–¥–µ –≤–∑—è—Ç–æ –∑ –º–µ—Ç–∞–¥–∞–Ω–∏—Ö)
    device = torch.device("cpu")
    model = load_model(SmallCNN, model_path, n_classes=None, device=device)
    model.eval()

    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ —É—Å–ø—ñ—à–Ω–æ")

    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∑—Ä–∞–∑–∫–∞
    waveform, sample_rate = create_stable_test_sample()
    print(f"‚úÖ –¢–µ—Å—Ç–æ–≤–∏–π –∑—Ä–∞–∑–æ–∫ —Å—Ç–≤–æ—Ä–µ–Ω–æ: {waveform.shape}")

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –º–æ–¥–µ–ª—ñ 5 —Ä–∞–∑—ñ–≤ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ
    predictions = []
    confidences = []

    print("\nüß™ –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ (5 —ñ—Ç–µ—Ä–∞—Ü—ñ–π)...")

    with torch.no_grad():
        for i in range(5):
            melspec = wav_to_melspec(waveform, sample_rate).unsqueeze(0).to(device)
            output = model(melspec)
            probabilities = torch.softmax(output, dim=1)
            predicted_class = torch.argmax(probabilities, dim=1).item()
            confidence = probabilities[0, predicted_class].item()

            predictions.append(predicted_class)
            confidences.append(confidence)

            print(f"  –Ü—Ç–µ—Ä–∞—Ü—ñ—è {i+1}: –∫–ª–∞—Å={CLASSES[predicted_class]}, –¥–æ–≤—ñ—Ä–∞={confidence:.4f}")

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ
    is_consistent = len(set(predictions)) == 1
    avg_confidence = np.mean(confidences)
    std_confidence = np.std(confidences)

    print("\n" + "=" * 60)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–ò –í–ê–õ–Ü–î–ê–¶–Ü–á")
    print("=" * 60)
    print(f"–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å: {'‚úÖ PASS' if is_consistent else '‚ùå FAIL'}")
    print(f"–°–µ—Ä–µ–¥–Ω—è –¥–æ–≤—ñ—Ä–∞: {avg_confidence:.4f}")
    print(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è –¥–æ–≤—ñ—Ä–∏: {std_confidence:.6f}")

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –º–æ–¥–µ–ª—ñ
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"\nüìà –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ:")
    print(f"  –í—Å—å–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤: {total_params:,}")
    print(f"  –¢—Ä–µ–Ω–æ–≤–∞–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤: {trainable_params:,}")

    # –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
    validation_results = {
        "model_path": model_path,
        "model_size_mb": os.path.getsize(model_path) / 1024 / 1024,
        "consistency_check": is_consistent,
        "predictions": [CLASSES[p] for p in predictions],
        "confidences": [float(c) for c in confidences],
        "avg_confidence": float(avg_confidence),
        "std_confidence": float(std_confidence),
        "total_parameters": total_params,
        "trainable_parameters": trainable_params,
        "status": "PASS" if is_consistent else "FAIL"
    }

    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    with open("validation_report.json", "w") as f:
        json.dump(validation_results, f, indent=2)

    print("\n‚úÖ –ó–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ: validation_report.json")

    # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è Markdown –∑–≤—ñ—Ç—É
    generate_markdown_report(validation_results)

    # –ü–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è exit code
    return 0 if is_consistent else 1


def create_stable_test_sample():
    """
    –°—Ç–≤–æ—Ä—é—î —Å—Ç–∞–±—ñ–ª—å–Ω–∏–π —Ç–µ—Å—Ç–æ–≤–∏–π –∑—Ä–∞–∑–æ–∫ (—Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–∏–π)
    –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª—ñ
    """
    # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–∏–Ω—É—Å–æ—ó–¥–∞–ª—å–Ω–∏–π —Å–∏–≥–Ω–∞–ª –∑ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏–º seed
    torch.manual_seed(42)
    sample_rate = 16000
    duration = 1.0  # 1 —Å–µ–∫—É–Ω–¥–∞
    frequency = 440.0  # A4 note

    t = torch.linspace(0, duration, int(sample_rate * duration))
    waveform = torch.sin(2 * np.pi * frequency * t).unsqueeze(0)

    return waveform, sample_rate


def generate_markdown_report(results: dict):
    """–ì–µ–Ω–µ—Ä—É—î Markdown –∑–≤—ñ—Ç"""

    report = f"""# üîç –ó–≤—ñ—Ç –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó –º–æ–¥–µ–ª—ñ

## –ó–∞–≥–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è

- **–ú–æ–¥–µ–ª—å:** `{results['model_path']}`
- **–†–æ–∑–º—ñ—Ä:** {results['model_size_mb']:.2f} MB
- **–°—Ç–∞—Ç—É—Å:** {'‚úÖ PASS' if results['status'] == 'PASS' else '‚ùå FAIL'}

## –¢–µ—Å—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ

–ú–æ–¥–µ–ª—å –±—É–ª–∞ –ø—Ä–æ—Ç–µ—Å—Ç–æ–≤–∞–Ω–∞ 5 —Ä–∞–∑—ñ–≤ –Ω–∞ –æ–¥–Ω–æ–º—É —ñ —Ç–æ–º—É –∂ —Å—Ç–∞–±—ñ–ª—å–Ω–æ–º—É –∑—Ä–∞–∑–∫—É.

### –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è:
"""

    for i, (pred, conf) in enumerate(zip(results['predictions'], results['confidences']), 1):
        report += f"{i}. **{pred}** (–¥–æ–≤—ñ—Ä–∞: {conf:.4f})\n"

    report += f"""
### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:

- **–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ñ—Å—Ç—å:** {'‚úÖ –í—Å—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –æ–¥–Ω–∞–∫–æ–≤—ñ' if results['consistency_check'] else '‚ùå –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –≤—ñ–¥—Ä—ñ–∑–Ω—è—é—Ç—å—Å—è'}
- **–°–µ—Ä–µ–¥–Ω—è –¥–æ–≤—ñ—Ä–∞:** {results['avg_confidence']:.4f}
- **–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è:** {results['std_confidence']:.6f}

## –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ

- **–í—Å—å–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤:** {results['total_parameters']:,}
- **–¢—Ä–µ–Ω–æ–≤–∞–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤:** {results['trainable_parameters']:,}

---
*–ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ GitHub Actions pipeline*
"""

    with open("validation_report.md", "w", encoding="utf-8") as f:
        f.write(report)

    print("‚úÖ Markdown –∑–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ: validation_report.md")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–í–∞–ª—ñ–¥–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ")
    parser.add_argument("--model-path", type=str, required=True,
                        help="–®–ª—è—Ö –¥–æ –º–æ–¥–µ–ª—ñ (.pt)")

    args = parser.parse_args()

    exit_code = validate_model_consistency(args.model_path)
    exit(exit_code)
